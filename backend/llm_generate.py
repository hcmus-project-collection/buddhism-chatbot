import logging
import os

import openai

from dotenv import load_dotenv

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

load_dotenv()

OPENAI_API_BASE = os.getenv(
    "OPENAI_API_BASE",
    "https://api.openai.com/v1",
)  # In case we need to use a different OpenAI-compatible API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "no-need")
OPENAI_MODEL_NAME = os.getenv(
    "OPENAI_MODEL_NAME",
    "gpt-4o-mini",
)

client = openai.OpenAI(
    base_url=OPENAI_API_BASE,
    api_key=OPENAI_API_KEY,
)

SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng.

B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p c√¢u h·ªèi (question), c√°c n·ªôi dung li√™n quan (relevant texts) (n·∫øu c√≥), v√† h∆∞·ªõng d·∫´n (instruction).

B·∫°n s·∫Ω d·ª±a v√†o c√°c n·ªôi dung li√™n quan (relevant texts) v√† h∆∞·ªõng d·∫´n (instruction) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (question).

N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng (nh∆∞ gi√° BTC h√¥m nay bao nhi√™u, Python l√† g√¨, ...), h√£y tr·∫£ l·ªùi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng c√¢u h·ªèi n√†y kh√¥ng li√™n quan ƒë·∫øn lƒ©nh v·ª±c chuy√™n m√¥n c·ªßa b·∫°n.

N·∫øu n·ªôi dung li√™n quan kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y (l√† r·ªóng ho·∫∑c kh√¥ng c√≥), h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y t·ª´ kho ng·ªØ li·ªáu.
""".strip()

USER_PROMPT_TEMPLATE = """
C√¢u h·ªèi: {question}

C√°c n·ªôi dung li√™n quan:
{relevant_texts}

H∆∞·ªõng d·∫´n: h√£y d·ª±a tr√™n c√°c n·ªôi dung li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

""".strip()


def generate_answer(
    question: str,
    relevant_texts: str | list[str],
    model_name: str = OPENAI_MODEL_NAME,
) -> str:
    if isinstance(relevant_texts, list):
        relevant_texts = "\n".join(f"- {text}" for text in relevant_texts)

    prompt = USER_PROMPT_TEMPLATE.format(
        question=question,
        relevant_texts=relevant_texts,
    )
    logger.info(f"Calling LLM at {OPENAI_API_BASE} with model {model_name}")
    logger.info(f"Prompt: {prompt}")
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt},
            ],
            temperature=0.1,
        )
    except Exception as e:
        logger.error(f"Error generating answer: {e}")
        return ""

    return response.choices[0].message.content or ""


if __name__ == "__main__":
    retrieved = [
        "Ng∆∞·ªùi l√†m thi·ªán ƒë∆∞·ª£c ph√∫c, ng∆∞·ªùi l√†m √°c ch·ªãu b√°o ·ª©ng.",
        "ƒê·∫ø Qu√¢n d·∫°y r·∫±ng nh√¢n qu·∫£ kh√¥ng sai m·ªôt m·∫£y may.",
    ]
    question = "ƒê·∫ø Qu√¢n d·∫°y ƒëi·ªÅu g√¨ v·ªÅ nh√¢n qu·∫£?"
    response = generate_answer(question, retrieved)
    logger.info(f"ü§ñ LLM said that: {response}")
