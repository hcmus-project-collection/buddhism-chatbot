import openai

from typing import Generator
from loguru import logger

from config import OPENAI_API_BASE, OPENAI_API_KEY, OPENAI_MODEL_NAME

client = openai.OpenAI(
    base_url=OPENAI_API_BASE,
    api_key=OPENAI_API_KEY,
)

SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng.

B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p c√¢u h·ªèi (question), c√°c n·ªôi dung li√™n quan (relevant texts) (n·∫øu c√≥), v√† h∆∞·ªõng d·∫´n (instruction).

B·∫°n s·∫Ω d·ª±a v√†o c√°c n·ªôi dung li√™n quan (relevant texts) v√† h∆∞·ªõng d·∫´n (instruction) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (question).

N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng (nh∆∞ gi√° BTC h√¥m nay bao nhi√™u, Python l√† g√¨, ...), h√£y tr·∫£ l·ªùi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng c√¢u h·ªèi n√†y kh√¥ng li√™n quan ƒë·∫øn lƒ©nh v·ª±c chuy√™n m√¥n c·ªßa b·∫°n.

N·∫øu n·ªôi dung li√™n quan kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y (l√† r·ªóng ho·∫∑c kh√¥ng c√≥), h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y t·ª´ kho ng·ªØ li·ªáu.
""".strip()

USER_PROMPT_TEMPLATE = """
C√¢u h·ªèi: {question}

C√°c n·ªôi dung li√™n quan:
{relevant_texts}

H∆∞·ªõng d·∫´n: h√£y d·ª±a tr√™n c√°c n·ªôi dung li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

""".strip()


def generate_answer(
    question: str,
    relevant_texts: str | list[str],
    model_name: str = OPENAI_MODEL_NAME,
    stream: bool = True,
) -> str:
    if isinstance(relevant_texts, list):
        relevant_texts = "\n".join(f"- {text}" for text in relevant_texts)

    prompt = USER_PROMPT_TEMPLATE.format(
        question=question,
        relevant_texts=relevant_texts,
    )
    logger.info(f"Calling LLM at {OPENAI_API_BASE} with model {model_name}")
    logger.info(f"Prompt: {prompt}")
    if stream:
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
            )
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return ""

        return response.choices[0].message.content or ""

    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=0.1,
        stream=True,
    )


if __name__ == "__main__":
    retrieved = [
        "Ng∆∞·ªùi l√†m thi·ªán ƒë∆∞·ª£c ph√∫c, ng∆∞·ªùi l√†m √°c ch·ªãu b√°o ·ª©ng.",
        "ƒê·∫ø Qu√¢n d·∫°y r·∫±ng nh√¢n qu·∫£ kh√¥ng sai m·ªôt m·∫£y may.",
    ]
    question = "ƒê·∫ø Qu√¢n d·∫°y ƒëi·ªÅu g√¨ v·ªÅ nh√¢n qu·∫£?"
    response = generate_answer(question, retrieved)
    logger.info(f"ü§ñ LLM said that: {response}")
