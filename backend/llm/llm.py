import openai
from backend.config import OPENAI_API_BASE, OPENAI_API_KEY, OPENAI_MODEL_NAME
from loguru import logger

client = openai.OpenAI(
    base_url=OPENAI_API_BASE,
    api_key=OPENAI_API_KEY,
)

SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng.

B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p c√¢u h·ªèi (question), c√°c n·ªôi dung li√™n quan (relevant texts) (n·∫øu c√≥), v√† h∆∞·ªõng d·∫´n (instruction). ƒê·ªëi v·ªõi m·ªói n·ªôi dung li√™n quan, b·∫°n c≈©ng s·∫Ω ƒë∆∞·ª£c cung c·∫•p c√°c n·ªôi dung xu·∫•t hi·ªán tr√™n c√πng trang v·ªõi c√¢u ƒë√≥ (texts_on_the_same_page). C√°c n·ªôi dung n√†y s·∫Ω b·∫Øt ƒë·∫ßu b·∫±ng pattern 'C√°c n·ªôi dung li√™n quan tr√™n c√πng trang:'. H√£y xem x√©t c·∫©n th·∫≠n c√°c ph·∫ßn n√†y (bao g·ªìm c·∫£ c√°c n·ªôi dung li√™n quan - relevant texts v√† c√°c n·ªôi dung xu·∫•t hi·ªán tr√™n c√πng trang - texts_on_the_same_page), v√† d·ª±a v√†o ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√¥n gi√°o ph∆∞∆°ng ƒê√¥ng (nh∆∞ gi√° BTC h√¥m nay bao nhi√™u, Python l√† g√¨, ...), h√£y tr·∫£ l·ªùi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng c√¢u h·ªèi n√†y kh√¥ng li√™n quan ƒë·∫øn lƒ©nh v·ª±c chuy√™n m√¥n c·ªßa b·∫°n.

N·∫øu n·ªôi dung li√™n quan kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y (l√† r·ªóng ho·∫∑c kh√¥ng c√≥), h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y t·ª´ kho ng·ªØ li·ªáu.
""".strip()

USER_PROMPT_TEMPLATE = """
C√¢u h·ªèi: {question}

C√°c n·ªôi dung li√™n quan:
{relevant_texts}

H∆∞·ªõng d·∫´n: h√£y d·ª±a tr√™n c√°c n·ªôi dung li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

""".strip()


def generate_answer(
    question: str,
    relevant_texts: list[dict],
    model_name: str = OPENAI_MODEL_NAME,
    stream: bool = True,
) -> str:
    relevant_texts_str = "\n"
    for text in relevant_texts:
        relevant_texts_str += f"- {text['text']}\n"
        relevant_texts_str += "C√°c n·ªôi dung li√™n quan tr√™n c√πng trang:\n"
        for same_page_text in text["texts_on_the_same_page"]:
            relevant_texts_str += f"- {same_page_text['text']}\n"
        relevant_texts_str += "\n"

    prompt = USER_PROMPT_TEMPLATE.format(
        question=question,
        relevant_texts=relevant_texts_str,
    )
    logger.info(f"Calling LLM at {OPENAI_API_BASE} with model {model_name}")
    logger.info(f"Prompt: {prompt}")
    if stream:
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
            )
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return ""

        return response.choices[0].message.content or ""

    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=0.1,
        stream=True,
    )


if __name__ == "__main__":
    retrieved = [
        "Ng∆∞·ªùi l√†m thi·ªán ƒë∆∞·ª£c ph√∫c, ng∆∞·ªùi l√†m √°c ch·ªãu b√°o ·ª©ng.",
        "ƒê·∫ø Qu√¢n d·∫°y r·∫±ng nh√¢n qu·∫£ kh√¥ng sai m·ªôt m·∫£y may.",
    ]
    question = "ƒê·∫ø Qu√¢n d·∫°y ƒëi·ªÅu g√¨ v·ªÅ nh√¢n qu·∫£?"
    response = generate_answer(question, retrieved)
    logger.info(f"ü§ñ LLM said that: {response}")
